{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Predicting Customer Churn in Telco\nThis notebook is forked from the original notebook (https://github.com/elenalowery/DSX-Local-Telco-Churn/blob/master/Notebooks/Telco%20Churn%20ML_Local.ipynb) which was created for DSX Local (Data Science Experience running locally) and ported to work with Watson Studio on the IBM Cloud.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Verify Python 3.5\nimport platform\nprint(platform.python_version())"
        }, 
        {
            "source": "In this notebook you will learn how to build a predictive model with [Apache Spark](https://spark.apache.org/) machine learning API (SparkML) and deploy it for scoring using [Machine Learning](https://console.bluemix.net/catalog/services/machine-learning) (ML) service.\n\nThis notebook walks you through these steps:\n- Load and Visualize data set.\n- Build a predictive model with SparkML API\n- Save the model in the ML repository\n- Create a Deployment in ML (via UI)\n- Test the model (via UI)\n- Test the model (via REST API)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Step 1: Review Use Case", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The analytics use case implemented in this notebook is telco churn. While it's a simple use case, it implements all steps from the CRISP-DM methodolody, which is the recommended best practice for implementing predictive analytics. \n![CRISP-DM](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png)\n\nThe analytics process starts with defining the business problem and identifying the data that can be used to solve the problem. For Telco churn, we use demographic and historical transaction data. We also know which customers have churned, which is the critical information for building predictive models. In the next step, we use visual APIs for data understanding and complete some data preparation tasks. In a typical analytics project data preparation will include more steps (for example, formatting data or deriving new variables). \n\nOnce the data is ready, we can build a predictive model. In our example we are using the SparkML Random Forrest classification model. Classification is a statistical technique which assigns a \"class\" to each customer record (for our use case \"churn\" or \"no churn\"). Classification models use historical data to come up with the logic to predict \"class\", this process is called model training. After the model is created, it's usually evaluated using another data set. \n\nFinally, if the model's accuracy meets the expectations, it can be deployed for scoring. Scoring is the process of applying the model to a new set of data. For example, when we receive new transactional data, we can score the customer for the risk of churn.  \n\nWe also developed a sample Python Flask application to illustrate deployment: http://predictcustomerchurn.mybluemix.net/. This application implements the REST client call to the model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Working with Notebooks", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "If you are new to Notebooks, here's a quick overview of how to work in this environment.\n\n1. The notebook has 2 types of cells - markdown (text) and code. \n2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Step 2: Load data ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For this notebook, we will leverage two csv files:\n- customer.csv: this file provides information about the customer such as gender, age, marital status, number of children, income, calling plans, and usage.\n- churn.csv: this provides a historical record of which customers have churned.\n\nYou will need to download these files from the following github repository and upload to your Cloud Object Storage instance associated with your project.\n1. Download churn.csv and customer.csv files from the following repository to your local machine: https://github.com/elenalowery/DSX-Local-Telco-Churn/blob/master/data/\n2. Upload to your Cloud Object Storage instance by following these steps:\n    - Click on the **Data** icon (top right)\n    - Selet the **Files** tab\n    - Either drop the churn.csv and customer.csv file or click browse and select those files from your local machine.\n    \nAt this point, you should have both of these csv files in your Cloud Object storage.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Next, we'll need to load the data from the csv files into Spark data frames. To do so, click the **Insert to code** under the **customer.csv** file and click **Insert SparkSession DataFrame**.\n\nThis will insert code in a notebook cell that would load the csv file into a Spark data frame. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "source": "Repeat the same process to load **churn.csv** file into another data frame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Copy the dataframes into generic dataframes with more meaningful names\n# Note that your data frames may have slightly different names (maybde df_data_3 and df_data_4)\ndf_customer = df_data_1\ndf_customer_churn = df_data_2"
        }, 
        {
            "source": "If the first step ran successfully (you saw the output), then continue reviewing the notebook and running each code cell step by step. Note that not every cell has a visual output. The cell is still running if you see a * in the brackets next to the cell. \n\nIf the first step didn't finish successfully, check with the instructor. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Step 3: Merge Files\nJoin the two tables (customer information and churn information) based on the ID field.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data=df_customer.join(df_customer_churn,df_customer['ID']==df_customer_churn['ID']).select(df_customer['*'],df_customer_churn['CHURN'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print how many records the data includes\ndata.count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print top 5 records\ndata.head(5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print the data schema\ndata.printSchema()"
        }, 
        {
            "source": "### Step 4: Rename some columns\nThis step is to remove spaces from columns names, it's an example of data preparation that you may have to do before creating a model. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data = data.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\n# If you need to change a column type from String to double\n# from pyspark.sql.functions import col\n#data = data.withColumn(\"EstIncome\",col(\"Est Income\").cast(\"double\"))\ndata.toPandas().head()"
        }, 
        {
            "source": "### Step 5: Data understanding", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Data preparation and data understanding are the most time-consuming tasks in the data mining process. The data scientist needs to review and evaluate the quality of data before modeling.\n\nVisualization is one of the ways to reivew data.\n\nThe Brunel Visualization Language is a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and business users. \nMore information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n\nTry Brunel visualization here: http://brunel.mybluemix.net/gallery_app/renderer", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import brunel\ndf = data.toPandas()\n%brunel data('df') bar x(CHURN) y(EstIncome) mean(EstIncome) color(LocalBilltype) stack tooltip(EstIncome) | x(LongDistance) y(Usage) point color(Paymethod) tooltip(LongDistance, Usage) :: width=1100, height=400 "
        }, 
        {
            "source": "**PixieDust** is a Python Helper library for Spark IPython Notebooks. One of it's main features are visualizations. You'll notice that unlike other APIs which produce just output, PixieDust creates an **interactive UI** in which you can explore data.\n\nMore information about PixieDust: https://github.com/ibm-cds-labs/pixiedust?cm_mc_uid=78151411419314871783930&cm_mc_sid_50200000=1487962969", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "no_margin": "true", 
                        "orientation": "vertical", 
                        "title": "Usage by RatePlan", 
                        "chartsize": "50", 
                        "mpld3": "false", 
                        "aggregation": "AVG", 
                        "filter": "{\"value\": \"Budget\", \"regex\": \"false\", \"field\": \"LocalBilltype\", \"constraint\": \"None\", \"case_matter\": \"false\"}", 
                        "rowCount": "500", 
                        "handlerId": "barChart", 
                        "valueFields": "EstIncome", 
                        "rendererId": "matplotlib", 
                        "keyFields": "RatePlan"
                    }
                }
            }, 
            "outputs": [], 
            "source": "from pixiedust.display import *\ndisplay(data)"
        }, 
        {
            "source": "### Step 6: Build the Spark pipeline and the Random Forest model\n\"Pipeline\" is an API in SparkML that's used for building models.\nAdditional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Prepare string variables so that they can be used by the decision tree algorithm\n# StringIndexer encodes a string column of labels to a column of label indices\nSI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\nSI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\nSI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\nSI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\nSI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\nSI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\nlabelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data)\n\n# Pipelines API requires that input variables are passed in  a vector\n#assembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n#                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n#                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")\n\nassembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n                                       \"LongDistanceBilltypeEncoded\"], outputCol=\"features\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# instantiate the algorithm, take the default settings\nrf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n\npipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,labelIndexer, assembler, rf, labelConverter])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Split data into train and test datasets\ntrain, test = data.randomSplit([0.8,0.2], seed=6)\ntrain.cache()\ntest.cache()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Build models\nmodel = pipeline.fit(train)"
        }, 
        {
            "source": "### Step 7: Score the test data set", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "results = model.transform(test)\n#results=results.select(results[\"ID\"],results[\"CHURN\"],results[\"label\"],results[\"predictedLabel\"],results[\"prediction\"],results[\"probability\"])\n\nresults.toPandas().head(6)"
        }, 
        {
            "source": "### Step 8: Model Evaluation ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "precision = results.filter(results.label == results.prediction).count() / float(results.count())\nprint('Precision model1 = {0:.2f}'.format(precision))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\nprint('Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results)))"
        }, 
        {
            "source": "We have finished building and testing a predictive model. The next step is to deploy it for real time scoring. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Step 9: Save Model in ML repository\nAfter creating the predictive ML model, save it to your Machine Learning service. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Load required libraries\nfrom repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact\n\n# You need to create a Watson Machine Learning instance in IBM Cloud\n# Read WML credentials."
        }, 
        {
            "source": "In the next cell, insert the credentials for your Machine Learning service that you created on IBM Cloud.\n\nwml_credentials = {\n\n   \"url\": \"\",\n   \n   \"access_key\": \"\",\n   \n   \"username\": \"\",\n   \n   \"password\": \"\",\n   \n   \"instance_id\": \"\"\n   \n}", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Copy your WML credentials here\nwml_credentials={\n  \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n  \"access_key\": \"\",\n  \"username\": \"\",\n  \"password\": \"\",\n  \"instance_id\": \"\"\n}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create ML repo client and authenticate using WML credentials\nml_repository_client = MLRepositoryClient(wml_credentials['url'])\nml_repository_client.authorize(wml_credentials['username'], wml_credentials['password'])\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create model in WML; baseClustersFit is the trained model and training_data is the data used for training the model\nmodel_artifact = MLRepositoryArtifact(model, training_data=train, name=\"churn1\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create model in WML; model_rf is the trained model and train_data is the data used for training the model\nsaved_model = ml_repository_client.models.save(model_artifact)"
        }, 
        {
            "source": "## Step 10: Deploy model using UI and Test model using REST API or UI\n1. Save the notebook and switch to the **Models** tab of the project (**hint**: right click the project name link at the top, and open with another tab in your browser). \n2. Under **Models**, find and click into your saved model (note the model name you used above, in our example it is \"churn\")\n3. The **Overview** tab provides general information about trained model.\n4. The **Evaluation** tab shows information about evaluation results of trained model. Initially it is emtpy since it is not evaluated yet.\n5. Click the **Deployments** tab and select **Add Deployment**. This provides 3 options:\n   - Web Service: This provides a REST API endpoint to run the trained model against on new data.\n   - Batch Prediction: Run the trained model against a dataset in batch. Dataset could be read from Object Storage or DB2.\n   - Real Time Streaming: Run the trained model against data from Message Hub.\n  Select the Web Service option, provide a name for the web service and click Save.\n6. Once deployed, you click the model and that loads the page with information on deployed model. Specifically:\n   - **Overview**: Shows general information on deployed model.\n   - **Implementation**: Provides the endpoints of the deployed model as well as code snippets in multiple languages to actually make a REST call against the trained model. This also includes the required authentication when making the call.\n   - **Test**: Test the trained model via UI using the following values:\n`ID=99, Gender=M, Status=S, Children=0, Est Income=60000, Car Owner=Y, Age=34, LongDistance=68, International=50, Local=100, Dropped=0, Paymethod=CC, LocalBilltype=Budget, LongDistanceBilltype=Intnl_discount, Usage=334, RatePlan=3`\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Test model using REST API\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Go to Models, select the model you just trained (churn), then go to Implementation and get \n# the code snippet in Python\n\nimport urllib3, requests, json\n\n# retrieve your wml_service_credentials_username, wml_service_credentials_password, and wml_service_credentials_url from the\n# Service credentials associated with your IBM Cloud Watson Machine Learning Service instance\n\n\n\nheaders = urllib3.util.make_headers(basic_auth='{username}:{password}'.format(username=wml_credentials['username'], password=wml_credentials['password']))\nurl = '{}/v3/identity/token'.format(wml_credentials['url'])\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n\n#payload_scoring = {\"fields\": [\"ID\", \"Gender\", \"Status\", \"Children\", \"EstIncome\", \"CarOwner\", \"Age\", \"LongDistance\", \"International\", \"Local\", \"Dropped\", \"Paymethod\", \"LocalBilltype\", \"LongDistanceBilltype\", \"Usage\", \"RatePlan\"], \"values\": [array_of_values_to_be_scored, another_array_of_values_to_be_scored]}\n\npayload_scoring = {\"fields\": [\"ID\", \"Gender\", \"Status\", \"Children\", \"EstIncome\", \"CarOwner\", \"Age\", \"LongDistance\", \"International\", \"Local\", \"Dropped\", \"Paymethod\", \"LocalBilltype\", \"LongDistanceBilltype\", \"Usage\", \"RatePlan\"], \"values\": [[\"99\", \"M\", \"S\", \"0\", \"60000\", \"Y\", \"34\", \"68\", \"50\", \"100\", \"0\", \"CC\", \"Budget\", \"Intnl_discount\", \"334\", \"3\"]]}\n\nscoring_endpoint = 'https://ibm-watson-ml.mybluemix.net/v3/wml_instances/13cc486f-01c7-4393-8032-f6d6fd0b599b/published_models/9cc07790-c8af-4ff7-8bfa-1ccb120944c7/deployments/ffee1a62-97c4-4f78-9cb1-416344ef8b04/online'\nresponse_scoring = requests.post(scoring_endpoint, json=payload_scoring, headers=header)\n\nprint(\"Scoring response\")\nprint(json.loads(response_scoring.text))"
        }, 
        {
            "source": "### (Optional) Step 11: Test Saved Model with Test UI\nOnce you've saved and deployed the machine learning model you created, there are multiple ways to test that model, one being using the REST api as shown in the cells above. Another approach is to use the Test UI of the deployed model as follows:\n- Click on the Test tab of the deployed model.\n- Enter some values for the various features (ID, Gender, Status, Children, EstIncome, CarOwner, Age, LongDistance, International, Local, Dropped, Paymethod, LocalBillType, LongDistanceBillType, Usage, Rate Plan)\n- Click **Predict** ==> this should return the predicted likelihoood whether that customer will churn or not", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Summary", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "You have finished working on this hands-on lab. In this notebook you created a model using SparkML API, deployed it in  Machine Learning service for online (real time) scoring and tested it using a test client. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\nCreated by **Sidney Phoon** and **Elena Lowery**\n<br/>\nyfphoon@us.ibm.com\nelowery@us.ibm.com\n<br/>\nJan 2, 2018\n\nPorted to Watson Studio on IBM Cloud by Joe Kozhaya\n<br/>kozhaya@us.ibm.com<br/>\nMarch 26, 2018", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}